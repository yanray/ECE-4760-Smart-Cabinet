<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <title>Robotic arm - Yanrui Wang and Mengde Wu - ECE 4760 - Cornell University</title>
    <link rel="stylesheet" href="css/bulma.min.css">
    <link rel="stylesheet" href="css/page.css">
    <style>
    .center {
      margin: auto;
      width: 60%;
      border: 3px solid #73AD21;
      padding: 10px;
    }
    </style>
  </head>
  <body>
    <nav class="navbar is-link is-fixed-top">
      <div class="container">
        <div class="navbar-brand">
          <a class="navbar-item" href="#">
            <img src="css/cornell_seal_simple_white.svg" alt="Cornell logo" height="40" width="40">
            Cornell ECE 4760
          </a>
        </div>
        <div class="navbar-menu">
          <div class="navbar-end">
            <span class="navbar-item">
              <a class="button is-primary" href="code_file.zip">
                Download Code
              </a>
            </span>
            <span class="navbar-item">
              <a class="button is-info is-inverted" href="https://github.com/yanray/Robotic-arm-">
                View on GitHub
              </a>
            </span>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero is-light is-medium is-bold">
      <div class="hero-body">
        <div class="container">
          <h1 class="title">Smart Cabinet</h1>
          <h2 class="subtitle">Yanrui Wang (yw2226) and Mengde Wu (mw934)</h2>
          <h2 class="subtitle">Friday, December 13, 2019</h2>
          <h2 class="subtitle">We designed a smart cabinet to store and grab toy cars automatically.</h2>
        </div>
      </div>
    </section>

    <section class="section" id="main-content">
      <div class="container">
        <div class="columns">
          <div class="column is-one-third" id="menu-container">
            <aside class="menu" id="menu">
              <ul class="menu-list">
                <li>
                  <a href="#Introduction" id="link_Introduction" class="is-active">Introduction</a>
                </li>
                <li>
                  <a href="#High Level Design" id="link_High_Level_Design" >High Level Design</a>
                </li>
                <li>
                  <a href="#Design" id="link_design">Design & Testing</a>
                  <ul>
                    <li><a href="#Hardware_Design" id="link_Hardware_Design">Hardware Design</a></li>
                    <li><a href="#Software_Design" id="link_Software_Design">Software Design</a></li>
                  </ul>
                <li>
                  <a href="#Results" id="link_Results" >Results</a>
                </li>
                <li>
                  <a href="#Conclusion" id="link_Conclusion" >Conclusion</a>
                </li>
                <li>
                  <a href="#Future_Work" id="link_Future_Work" >Future Work</a>
                </li>
                <li>
                  <a href="#work_distribution" id="link_work_distribution">Work Distribution</a>
                </li>
                <li>
                  <a href="#project_parts" id="link_project_parts">Project Parts</a>
                </li>
                <li>
                  <a href="#Code_Appendix" id="link_Code_Appendix">Code Appendix</a>
                </li>
                <li>
                  <a href="#References" id="link_References">References</a>
                </li>
              </ul>
            </aside>
          </div>
          <div class="column is-two-thirds">
            <div class="content">
              <iframe width="560" height="315" src="https://youtu.be/RgFg1tljnTs" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
              <h1 id="Introduction" class="checkpoint">Introduction</h1>
              <p>We designed a Smart Cabinet to store and grab toy cars automatically through pressing a 'store' button and a 'grabbing' button.</p>
              
              <h1 id="High Level Design" class="checkpoint">High Level Design</h1>
              <p>Our idea of final projet derived from a real problem in the world. It's very crowded in the big cities like New York City. When you drive in New York City, it's very crowded and very difficult to find an available parking space. How to solve this problem?   </p>
              <p>If we're able to design and build an intelligent parking lot underground, drivers only need to drive in specified location and check in. The car would be automatically stored underground. When drivers are back, they only need to check out before they go, the car would be shown up on the ground. Then all the problems are addressed. </p>
              <p>Based on this idea, we designed and built a Smart Cabinet to store and grab toy cars as a prototype to mimic the intelligent parking lot as we imagined. We designed two cells structure to store toy cars inside it. We designed a platform to put toy car on it, then press the 'store' button, and the system will store the item automatically in a cell. 
                After Smart Cabinet successfully stores an toy car, it will give a code on the TFT screen, which you need to remember in order to pick up toy car in the future.
                If you want to take the toy car from Smart Cabinet, you need to input the 'picking up' code and the system will automatically take the toy car out for you.</p>

              <h1 id="Design" class="checkpoint">Design</h1>
              <p> </p>
              <h2 id="Hardware_Design" class="checkpoint">Hardware Design</h2>
              <p> The main structure of Smart Cabinet is built by acrylic. We have many figures drawn in AutoCAD in order to do laser cutting. </p>
              <figure>
                <img src="img/all.png" alt="" width="800" height="800">
                <figcaption>Figure 1: All laser cutting design. </figcaption>
              </figure>
              <p><strong>Figure1 Note:  </strong>All laser cutting design is in 2D. We used 6mm acrylic to cut every side and assemble them together. The bulges and hollows structure are aimed to attach each side of acrylic firmly. </p>
      
              <p>On figure 1, we shown all laser cutting figures we drawn in the final project. In the following, we only shown bottom side, top side and front side specifically since they're most important. In addition, the other sides of Smart Cabinet has similar design with those three main parts.  </p> 
             
              <figure>
                <img src="img/bottom.png" alt="" width="400" height="400">
                <figcaption>Figure 2: Bottom Side of Smart Cabinet. </figcaption>
              </figure>
              <p><strong>Figure2 Note:  </strong>On bottom side, we designed 6 legs for Smart Cabinet. The top middle circle is to to be attached with the central stepper motor. The holes on its left and right is to be attached with 3D print material finally connect with two rods.</p>
      
              <figure>
                <img src="img/top.png" alt="" width="400" height="400">
                <figcaption>Figure 3: Top Side of Smart Cabinet. </figcaption>
              </figure>
              <p><strong>Figure3 Note:  </strong>On top side, we designed three holes to be attached with one lead screw and two rods. The bottom rectangle structure is to hold PIC32 and board of buttons-design. </p>
      
              <figure>
                <img src="img/front_side.png" alt="" width="400" height="400">
                <figcaption>Figure 4: Front Side of Smart Cabinet. </figcaption>
              </figure>
              <p><strong>Figure4 Note:  </strong> The middle two sigle holes are to be attached with two different motors in each cell structure. The right and left side structure are used to be attached with two rods. </p>
      
              <figure>
                <img src="img/woodcut.PNG" alt="" width="400" height="400">
                <figcaption>Figure 5: Wood Design for Wood Box. </figcaption>
              </figure>
              <p><strong>Figure5 Note:  </strong>This figure shows the laser cut file for building the wood box. This wood box consists of five parts. We glued these five parts together using wood glue. Then we just used wood screws to fix this wood box on a part printed by 3D-printer. Since this wood box does not have a bottom, it can be used to pull the item from the platform or push the item onto the platform.</p>

              <figure>
                <img src="img/platform.PNG" alt="" width="400" height="400">
                <figcaption>Figure 6: Platform deisgn of Smart Cabinet. </figcaption>
              </figure>
              <p><strong>Figure6 Note:  </strong> 3D print design of central platform.</p>
              
              <figure>
                <img src="img/box_part.PNG" alt="" width="400" height="400">
                <figcaption>Figure 7: Side Box Structure of Smart Cabinet. </figcaption>
              </figure>
              <p><strong>Figure7 Note:  </strong> 3D print design of side box structure.</p>
              
              <figure>
                <img src="img/motor_circuit.png" alt="" width="400" height="400">
                <figcaption>Figure 8: stepper motor driver circuit. </figcaption>
              </figure>
              <p><strong>Figure8 Note:  </strong> stepper motor driver manual.</p>
              
              <figure>
                <img src="img/shift-key.png" alt="" width="400" height="400">
                <figcaption>Figure 9: button design. </figcaption>
              </figure>
              <p><strong>Figure9 Note:  </strong> button circuit.</p>

              
              
              <h2 id="Software_Design" class="checkpoint">Software Design</h2>
              <p> The entire software design consisted of four threads. The overall purpose of each of these components is as follows: </[>
              <p> Timer thread: draw the user interface on TFT screen and keep updating status of system at fixed 10 fps </p>
              <p> Command thread: preset what the system needs to do (set the status of motors: which motor and the direction of rotation of that motor) given different commands and keep checking what command has been given </p>
              <p> Key thread: Keep checking which key has been pressed and released and give the corresponding command  </p>
              <p> Motor thread: Keep checking the status of motors, which is set in the command thread, and make the motors run according to the status</p>
            
              <h3>Stepper Motor Control</h3>
              <p> There are two kinds of stepper motor in our project. Since they use different kinds of stepper motor driver, the codes to drive them are different. </p>
              <p> For the stepper motors using ULN2003 driver board, we need to directly send pulse wave to the four phases of the motor. The code is shown below. </p>
              <div class="center">
                <p><b></b>                 
                            writePE(GPIOZ, 0x01);
                            PT_YIELD_TIME_msec(2);
                            writePE(GPIOZ, 0x02);
                            PT_YIELD_TIME_msec(2);
                            writePE(GPIOZ, 0x04);
                            PT_YIELD_TIME_msec(2);
                            writePE(GPIOZ, 0x08);
                            PT_YIELD_TIME_msec(2);</p>
              </div>   

            
              <figure>
                <div class="columns">
                  <div class="column">
                    <img src="img/image_capture.jpg" alt="Image recognition using OpenCV1">
                  </div>
                  <div class="column">
                    <img src="img/image_detection.png" alt="Image recognition using OpenCV2">
                  </div>
                </div>
                <figcaption>Figure 6: Image recognition using OpenCV. </figcaption>
              </figure>
              <p><strong>Figure6 Note:  </strong>The Pi camera takes a picture of the workspace (left) and saves the image in the RPi3 file system. Then the image recognition C++ program identifies the red circles. The identified cycles are outlined in green, and the center locations are printed for testing purposes (right). </p>
              
              
              <h3>Coordinate Conversion</h3>
              <p> The circles’ location calculated by OpenCV were in units of pixels, so we had to convert them to actual distances in meters. There were many ways to do this, such as external packages. Since our platform was not flat and the image was not a perfect projection of the actual platform, we thought the best way was to take some sample points and measure the backward projection. So we first found out the maximum range that the arm could reach, which was 0.15 m to -0.15 m in y (horizontal) direction and 0 m to 0.3 m in x (vertical) direction. Then we moved the arm to the two diagonal corners and placed the bottles in the place where the arm could grab. Next we took an image of the field and calculated the pixel position of the bottle’s center. Now we have two set of (x, y) points, one in meter and one in the corresponding pixels. We found the slope and y intercept for both x and y direction, and wrote a simple linear conversion function. finally, we converted the bottles’ centers from pixel to meter with this function and stored them in a text file. </p>
              <p> From what we saw in the end result, this simple linear conversion was very accurate in most cases. It was most accurate when the bottle was close to the center of the field (where the camera was placed) and less accurate further away. This was mostly because the wooden field was not flat, and the image of the camera was also not a perfect linear projection. A way to improve its accuracy was to sample more points and make a 2D interpolation using methods such as basis spline. Due to time limit, we settled with linear conversion.</p>
              <h3>Inverse Kinematics</h3>
              <figure>
                <img src="img/DH_matrix.jpg" alt="DH_matrix" width="450" height="450">
                <figcaption>Figure 7: DH_matrix. </figcaption>
              </figure>
              <p><strong>Figure7 Note:  </strong>On the left, we build different frames on each servo. On the right, we list four parameters for frame change. This aims to compute Forward Kinematics. </p>
              
              <p> The second task of this system was to convert the target location in meters to servo rotating degrees in angles. We used Inverse Kinematics to achieve this task. Before we dive into Inverse Kinematics, we should briefly introduce Forward Kinematics. Suppose we know the rotating angles of our 6 servos, we could build a reference frame for every servo. Forward Kinematics is to build a position relationship of base attached servo with end-effector, also known as a gripper. The basic method for Inverse Kinematics is initializing a rotating angle for each servo and then use Forward Kinematics to compute the current target position. We compare current target position and goal position to output an error. Then Velocity Kinematics aims to calculate the updated rotating angles (you could image Velocity Kinematics as gradient since we want to minimize error). Then in the iteration, we minimize error to a threshold and output rotating angles for each servo. </p>
              <p> We used open-loop control which meant the arm would reach the target with a single calculation. Although this was not as accurate as close-loop control, it was faster. Considering that both image recognition and serial communication were slow, we chose open-loop control. The test result showed that this method was fairly good in terms of speed and accuracy. On the simple model platform that we build, our robot could successfully recognize and pick up bottles over 95% of the time unless the bottle is out of the arm’s reach. Additionally, we used Forward Kinematics to test the accuracy of Inverse Kinematics, and the results were satisfactory. </p>
              <h3>Communication between Raspberry Pi 3 and Arduino </h3>
              <p> We used Serial Port to make Raspberry Pi 3 communicate to arduino. There were six values to be sent for each target location, one angle for each servo. Because the arduino had only one serial port, and it needed to communicate to both the RPi3 and the servos, we set up a new hardware serial on pin 10. To ensure everything was sent successfully and in order, we set up a simple protocol for the RPi3 and arduino to communicate back and forth. The arduino first send a ‘ready’ message to RPi3. Then RPi3 send the first value, and arduino had to confirm that it received this message by sending this value back to RPi3. When all six values were sent, arduino would sent a ‘done’ to RPi3. If there were more red bottles on the work space, RPi3 would be ready to send the next location. Otherwise, RPi3 python program would exit and the camera would take a new image of the platform to be processed. </p>
              <h3>Control 6 servos by Arduino </h3>
              <p> We used Software Serial Port on Arduino to control the 6 different servos. The basic rule was to restrict the rotating limits for every servo first. The rotation range was between 0 ~ 240 degrees. The minimum increment or accuracy for each servo was 0.24 radian. Each servo had a unique ID number and we could identify them by this number. In addition, we could control their rotating duration and rotating position. In the Arduino code, we pre-defined several functions like move_to_initial(), move_to() and move_to_bin(). They represent move to the vertical initial position, move to target location based on input arguments and move to bin location, respectively. During the testing, we did not face any problem with serial communication or servo control. </p>
              
              <h1 id="Results" class="checkpoint">Results</h1>
              <p>We reached our basic goals, and every subsystem performed well as expected. Our computer vision program recognized the red caps on the bottle with good accuracy. The coordinate conversion, although very simple, also performed very well within certain ranges of distances. Finally, our Inverse Kinematics algorithm was sturdy and very rarely failed unless the bottles were placed in unreachable places. The overall accuracy was over 95%. The bash script we wrote realized the whole process in a single command. In general, the whole process was very smooth, and we were very satisfied with the end product. </p>
              <p> Our minimum demo goal was to have the robotic arm touch the bottle. We achieved this goal and beyond. Our final system was able to grab the bottle and put it aside with a good success rate.</p>
               <figure>
                <div class="columns">
                  <div class="column">
                    <img src="img/grip1.jpg" alt="RecycleBot finding a bottle and putting it inside the recycle bin1">
                  </div>
                  <div class="column">
                    <img src="img/grip2.jpg" alt="RecycleBot finding a bottle and putting it inside the recycle bin2">
                  </div>
                </div>
                <figcaption>Figure 8: RecycleBot finding a bottle and putting it inside the recycle bin. </figcaption>
               </figure>
               <p><strong>Figure8 Note:  </strong>Those two image demonstrate how RecycleBot successfully identifies a bottle, grabs it at the target location, and puts it inside the recycle bin.</p>
              
              
              <h1 id="Conclusion" class="checkpoint">Conclusion</h1>
              <p>The test results that our system achieved were fairly good in terms of speed and accuracy. On the simple model platform that we build, our robot could successfully recognize and pick up bottles over 95% of the time. There were some situations when the robot performed poorly, such as when the light was dim and when there were other red objects on the platform. One thing we discovered is that it is impossible for the robot to be accurate 100% of the time. During the entire process, there were many places that could be heavily affected by noises and uncertainties, such as the image recognition stage, the inverse kinematic calculation, and so on. </p>

              <h1 id="Future_Work" class="checkpoint">Future Work</h1>
              <p> Our project has tremendous potential and many more additional features to be realized. There are two major directions of possible improvements. One is to make the model more realistic. This can be achieved by adding a more complete set of trash to our field space. Another direction is to make the system more accurate. This can be achieved by changing the control from open-loop to close-loop. </p>
              <p> In this project, we used milk bottles to represent recyclable wastes and white blocks to represent everything else. We also set our target position height to be constant, so the robot can only grab bottles that are standing up straight. To make this model more realistic, we can use a more complex machine learning program to recognize all types of recyclable wastes, such as other types of bottles, plastic bags, or even paper boxes. We also need the system to recognize those wastes in all directions, instead of just from the top view. To achieve this, we need to add more than one camera to also detect object’s height. </p>
              <p> With a more complex set of objects to grab, open-loop control will not be accurate enough. We need to change the picking up angles depending on the current location of end-effector and target location, orientation and shape. This change will be very complicated, corresponding solutions will be based on issues and how current industrial and research are handing them.  </p>
              <p> One more idea is proposed by Professor Joseph Skovira, we could change our fixed workspace to a conveyor belt. Robotic arm would pick up recyclable wastes sequencely. To realize this change, we need to make our system a real timed system. Computation complex and system working speed should be considered and improved.  </p>
              
              <h1 id="work_distribution" class="checkpoint">Work Distribution</h1>
              <p> Team work: testing, final report </p>
              <p> Yanrui Wang: mechanical assembly, Inverse Kinematics, arm control</p>
              <p> Yazhi Fan: OpenCV, coordinate conversion, serial communication </p>
              
              <h1 id="project_parts" class="checkpoint">Project Parts</h1>
              <table>
                <thead>
                  <tr>
                    <th>Part Name</th>
                    <th>Price (USD)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>LewanSoul LX-16A Serial Bus Servo</td>
                    <td>14.99 x 5 = 74.95</td>
                  </tr>
                  <tr>
                    <td>LewanSoul arm mechanical parts</td>
                    <td>5</td>
                  </tr>
                  <tr>
                    <td>Raspberry Pi Camera Module V2-8 Megapixel</td>
                    <td>20</td>
                  </tr>
                  <tr>
                    <td>old Arduino</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <td>RPi3 (provided)</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <td>scrap wood</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <td>total</td>
                    <td>99.95 (There are 6 servos on the arm but one is not in use, its value is always 0)</td>
                  </tr>
                </tbody>
              </table>
              
              <h1 id="Code_Appendix" class="checkpoint">Code Appendix</h1>
              <table>
                <thead>
                  <tr>
                    <th>Purpose</th>
                    <th>File Name</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Image recognition</td>
                    <td>circle_detect.cpp</td>
                  </tr>
                  <tr>
                    <td>Inverse Kinematics</td>
                    <td>final_code.py, IK.py, FK.py, VK.py</td>
                  </tr>
                  <tr>
                    <td>Servo control</td>
                    <td>ServoRotation.ino</td>
                  </tr>
                  <tr>
                    <td>bash script for running everything autonomously</td>
                    <td>arm.sh, arm_infinite.sh</td>
                  </tr>
                </tbody>
              </table>
              <p> Download is avaliable on the top of the web page</p>
              
              <h1 id="References" class="checkpoint">References</h1>
              <p> Thanks to Angus Gibbs (aag233) and Joao Pedro Carvao (jc2697) for giving us permission to use their web page design as a template. </p>
              <p> Thanks to Professor Joseph Skovira and TAs for giving us advice and help. </p>
              <ul>
                <li><a href="https://news.nationalgeographic.com/2017/07/plastic-produced-recycling-waste-ocean-trash-debris-environment/">https://news.nationalgeographic.com/2017/07/plastic-produced-recycling-waste-ocean-trash-debris-environment/</a></li>
                <li><a href="https://www.thingiverse.com/thing:1693444">https://www.thingiverse.com/thing:1693444</a></li>
                <li><a href="https://solarianprogrammer.com/2015/05/08/detect-red-circles-image-using-opencv/">https://solarianprogrammer.com/2015/05/08/detect-red-circles-image-using-opencv/</a></li>
                <li><a href="https://rpal.cs.cornell.edu/foundations/kinematics.pdf#section.3.8">https://rpal.cs.cornell.edu/foundations/kinematics.pdf#section.3.8</a></li>
                <li><a href="https://courses.ece.cornell.edu/ece5990/ECE5725_Spring2018_Projects/aag233_jc2697/index.html#design">https://courses.ece.cornell.edu/ece5990/ECE5725_Spring2018_Projects/aag233_jc2697/index.html#design</a></li>
                <li><a href="https://www.sunfounder.com/blog/rpi-ard/">https://www.sunfounder.com/blog/rpi-ard/</a></li>
                <li><a href="https://www.w3schools.com/html/">https://www.w3schools.com/html/</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <script src="css/page.js"></script>
  </body>
</html>
